In this section, we first describe existing work on bug localization in Section~\ref{sec.bugloc}. Next, we present existing work that also deal with cold-start problem in software engineering in Section~\ref{sec.crossproj}. Finally, we describe recent effort in software engineering that adapts deep learning to software engineering.

\subsection{Bug Localization}\label{sec.bugloc}

\dl{Ferdian, please identify more related papers and provide their descriptions below. Please also classify the approach into supervised and unsupervised.}

A number of papers have proposed various techniques that take as input a bug report and return a ranked list of source code files that are relevant to it~\cite{lukins2008source,RaoK11,SahaLKP14,rao2013incremental,huo2016learning}. These {\em text-based} bug localization techniques can be divided into two general families: supervised approaches~\cite{huo2016learning} and unsupervised ones~\cite{rao2013incremental}. Supervised approaches learn a model from data of bug reports whose relevant buggy source code files have been identified. Unsupervised approaches do not learn such model. We briefly introduce approaches that belong to each family below.

\vspace{0.2cm}\noindent{\bf Unsupervised Approaches.}

\vspace{0.2cm}\noindent{\bf Supervised Approaches.}

\subsection{Cross-Project Learning}\label{sec.crossproj}

The problem of scarcity of labelled data for a target project (aka. cold-start problem) has been explored in several automated software engineering tasks~\cite{ZimmermannNGGM09,TurhanMBS09,NamPK13,KitchenhamMT07}. Closest to our work, is the line of work on cross-project defect prediction~\cite{ZimmermannNGGM09,TurhanMBS09,NamPK13}. Note that defect prediction does not consider a target bug report, while bug localization takes as input a bug report and return files relevant to it. They are used in different software development phase, i.e., code inspection and testing (defect prediction) vs. debugging (bug localization), and thus are thus complementary with each other. We provide a description of existing work on cross-project defect prediction below.

Zimmermann et al. are among the first to investigate cross-project defect prediction~\cite{ZimmermannNGGM09}. They highlight that defect prediction works well if there is a sufficient amount of data from a project to train a model. However, they argue that sufficient data is often unavailable for many projects (especially new ones) and companies. One way to deal with the problem is to build a model from a project with sufficient data and use the model to predict defective code in another project -- which is referred to as cross-project defect prediction. To investigate viability of cross-project defect prediction, Zimmermann et al. consider 12 target projects and demonstrate that cross-project defect prediction is ``a serious challenge'' -- it is not possible to achieve good results by simply using models built from other projects. 

Zimmermann et al.'s study is a call-to-arms that spur active interest in the area of cross-project defect prediction. A number of solutions have been proposed to boost the effectiveness of cross-project defect prediction. These include the work by Turhan et al.~\cite{TurhanMBS09} and Nam et al.~\cite{NamPK13} highlighted below. 

Turhan et al. propose a relevancy filtering method to select training data that are closest to test data~\cite{TurhanMBS09}. In particular, they employ a k-nearest neighbor method to pick k training instances (i.e., files from a project with known defect labels) that are closest to each test data (i.e., files from a target project with unknown defect labels). The resultant training instances are then used to learn a model that is then applied to predict defect labels of files from the target project in the test data. The approach by Turhan et al. potentially omit many training instances, which may reduce the effectiveness of the resultant model. Nam et al. deal with cross-project defect prediction problem by leveraging the recent development in machine learning -- i.e., transfer learning~\cite{NamPK13}. In particular, they take an existing transfer learning method -- referred to as Transfer Component Analysis (TCA)~\cite{PanTKY11} -- and adapt it for defect prediction.

Following existing cross-project defect prediction studies, we first demonstrate that cross-project bug localization is a serious challenge (see RQ1 in Section~\ref{sec.exp}). We then propose a novel deep transfer learning method to deal with this challenge. We have also compared our solution with several adaptations of Turhan et al.'s relevancy filtering method and TCA~\cite{PanTKY11} for bug localization, and demonstrated that our solution outperforms these baselines.

%\dl{Ferdian, please include some existing work on cross-project defect prediction. One recent work by our group is~\cite{XiaLPNW16}.}

\subsection{Deep Learning in Software Engineering}\label{sec.deeplearning}

Recently, deep learning~\cite{Goodfellow-et-al-2016}, which is a recent breakthrough in machine learning domain, has been applied in many areas. Software engineering is not an exception. Our approach also employs deep learning. Thus, we review related studies that also employ deep learning to improve automated software engineering. In the process, we highlight the difference between our approach and the existing work, and thus stress our novelty.

\dl{Ferdian, please include some existing work on deep learning in SE. Some recent work include the following: \cite{WangLT16},~\cite{YangLXZS15},~\cite{0004CC17},~\cite{LeeHLKJ17},~\cite{XuYXXCL16}}.
