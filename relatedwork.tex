In this section, we first describe existing work on bug localization in Section~\ref{sec.bugloc}. Next, we present existing work that also deal with cold-start problem in software engineering in Section~\ref{sec.crossproj}. Finally, we describe recent effort in software engineering that adapts deep learning to software engineering.

\subsection{Bug Localization}\label{sec.bugloc}

%\dl{Ferdian, please identify more related papers and provide their descriptions below. Please also classify the approach into supervised and unsupervised.}

A number of papers have proposed various techniques that take as input a bug report and return a ranked list of source code files that are relevant to it~\cite{lukins2008source,rao2011retrieval,rao2013incremental,SahaLKP13,SahaLKP14,zhou2012should,huo2016learning}. These {\em text-based} bug localization techniques can be divided into two general families: supervised approaches~\cite{zhou2012should,huo2016learning} and unsupervised ones~\cite{lukins2008source,rao2011retrieval,rao2013incremental,SahaLKP13,SahaLKP14}. Supervised approaches learn a model from data of bug reports whose relevant buggy source code files have been identified. Unsupervised approaches do not learn such model. We briefly introduce some of the approaches that belong to each family below. Due to space limitation, our survey here is by no means complete.

\vspace{0.2cm}\noindent{\bf Unsupervised Approaches.} Lukins et al. apply Latent Dirichlet Allocation (LDA) to extract latent topics from source code files and bug reports~\cite{lukins2008source}. Given an input bug report, source code files that are similar in their topic distributions as the bug report are returned. Rao et al. investigate a number of generic and composite text retrieval models, e.g., Unigram Model (UM), Vector Space Model (VSM), Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), etc., for bug localization~\cite{rao2011retrieval}. Their empirical study demonstrates that simple text retrieval models, i.e., unigram model and vector space model, are performing the best. Saha et al. apply structured information retrieval to improve the performance of existing solutions further~\cite{SahaLKP13}. Their proposed approach, named BLUiR, separates text in the source code files and bug reports into different groups and compute similarities between the different groups separately before combining the similarity scores together. In particular, it separates text in source code files into class names, method names, identifier names and comments, and text in bug reports into summary and description. In a later work, Saha et al.  reports an extended evaluation of BLUiR with several thousand more bug reports~\cite{SahaLKP14}.

\vspace{0.2cm}\noindent{\bf Supervised Approaches.} Zhou et al. employs a modified Vector Space Model (i.e., rVSM) and makes use similar fixed bug reports to boost bug localization performance~\cite{zhou2012should}. Their proposed approach employs lazy learning; it stores past fixed bug reports and compares incoming bug reports to these historical bug reports. Source code files are then ranked based on how often they are fixed to address prior bug reports. Zhou et al. have shown that their proposed approach named Bug Locator outperforms many unsupervised approaches, e.g., VSM, SUM, LSI and LDA. More recently, Huo et al.~\cite{huo2016learning} extends Zhou et al.'s work by employing a eager learning approach based on Convolutional Neural Network (CNN)~\cite{kim2014convolutional}. They demonstrate that their proposed approach named NP-CNN outperforms Bug Locator.

In this work, we propose a novel deep transfer learning approach that is built on top of NP-CNN~\cite{kim2014convolutional}. To the best of our knowledge, we are the first to explore cross-project bug localization. We have also demonstrated that our proposed approach named DTBL outperforms NP-CNN for cross-project setting.

\subsection{Cross-Project Learning}\label{sec.crossproj}

The problem of scarcity of labelled data for a target project (aka. cold-start problem) has been explored in several automated software engineering tasks~\cite{ZimmermannNGGM09,TurhanMBS09,NamPK13,KitchenhamMT07}. Closest to our work, is the line of work on cross-project defect prediction~\cite{ZimmermannNGGM09,TurhanMBS09,NamPK13}. Note that defect prediction does not consider a target bug report, while bug localization takes as input a bug report and return files relevant to it. They are used in different software development phase, i.e., code inspection and testing (defect prediction) vs. debugging (bug localization), and thus are thus complementary with each other. We provide a description of existing work on cross-project defect prediction below. Due to space limitation, our survey here is by no means complete.

Zimmermann et al. are among the first to investigate cross-project defect prediction~\cite{ZimmermannNGGM09}. They highlight that defect prediction works well if there is a sufficient amount of data from a project to train a model. However, they argue that sufficient data is often unavailable for many projects (especially new ones) and companies. One way to deal with the problem is to build a model from a project with sufficient data and use the model to predict defective code in another project -- which is referred to as cross-project defect prediction. To investigate viability of cross-project defect prediction, Zimmermann et al. consider 12 target projects and demonstrate that cross-project defect prediction is ``a serious challenge'' -- it is not possible to achieve good results by simply using models built from other projects. 

Zimmermann et al.'s study is a call-to-arms that spur active interest in the area of cross-project defect prediction. A number of solutions have been proposed to boost the effectiveness of cross-project defect prediction. These include the work by Turhan et al.~\cite{TurhanMBS09} and Nam et al.~\cite{NamPK13} highlighted below. 

Turhan et al. propose a relevancy filtering method to select training data that are closest to test data~\cite{TurhanMBS09}. In particular, they employ a k-nearest neighbor method to pick k training instances (i.e., files from a project with known defect labels) that are closest to each test data (i.e., files from a target project with unknown defect labels). The resultant training instances are then used to learn a model that is then applied to predict defect labels of files from the target project in the test data. The approach by Turhan et al. potentially omit many training instances, which may reduce the effectiveness of the resultant model. Nam et al. deal with cross-project defect prediction problem by leveraging the recent development in machine learning -- i.e., transfer learning~\cite{NamPK13}. In particular, they take an existing transfer learning method -- referred to as Transfer Component Analysis (TCA)~\cite{PanTKY11} -- and adapt it for defect prediction.

Following existing cross-project defect prediction studies, we first demonstrate that cross-project bug localization is a serious challenge (see RQ1 in Section~\ref{sec.exp}). We then propose a novel deep transfer learning method to deal with this challenge. We have also compared our solution with several adaptations of Turhan et al.'s relevancy filtering method~\cite{TurhanMBS09} and TCA~\cite{PanTKY11} for bug localization, and demonstrated that our solution outperforms these baselines.

%\dl{Ferdian, please include some existing work on cross-project defect prediction. One recent work by our group is~\cite{XiaLPNW16}.}

\subsection{Deep Learning in Software Engineering}\label{sec.deeplearning}

Recently, deep learning~\cite{Goodfellow-et-al-2016}, which is a recent breakthrough in machine learning domain, has been applied in many areas. Software engineering is not an exception. Our approach is built upon the state-of-the-art bug localization technique employing deep learning~\cite{huo2016learning}. In this subsection, we briefly review some related studies that also employ deep learning to improve other automated software engineering tasks. In the process, we highlight the difference between our approach and the existing work, and thus stress our novelty. Due to space limitation, our survey here is by no means complete.

Yang et al. applies Deep Belief Network (DBN) to learn higher-level features from a set of basic features extracted from commits (e.g., lines of code added, lines of code deleted, etc.) to predict buggy commits~\cite{YangLXZS15}. Wang et al. applies Deep Belief Network (DBN) to tokens extracted from program Abstract Syntax Trees to better predict defective files~\cite{WangLT16}. Specifically, DBN is used to extract semantic vectors that are then used as input to a classifier to learn a model to differentiate defective from non-defective files. Guo et al. uses word embedding and one/two layers Recurrent Neural Network (RNN) to link software subsystem requirements (SSRS) to their corresponding software subsystem design descriptions (SSDD)~\cite{0004CC17}. They have evaluated their solution on 1,651 SSRS and 466 SSDD from an industrial software system. Xu et al. applies word embedding and convolutional neural network (CNN) to predict semantic links between knowledge units in Stack Overflow (i.e., questions and answers) to help developers better navigate and search the popular knowledge base~\cite{XuYXXCL16}. Lee et al. applies word embedding and CNN to identify developers that should be assigned to fix a bug report~\cite{LeeHLKJ17}.

While existing works mostly take an off-the-shelf deep learning algorithm (e.g., DBN, CNN, etc.) and apply it to solve their problem, in this work, we design a customized deep learning algorithm and demonstrates that it works better than off-the-shelf solutions. \dl{Xuan and Ming, if you have stronger points to highlight the novelty of our approach compared to the above papers, please kindly help to add it here :-)}

%\dl{Ferdian, please include some existing work on deep learning in SE. Some recent work include the following: \cite{WangLT16},~\cite{YangLXZS15},~\cite{0004CC17},~\cite{LeeHLKJ17},~\cite{XuYXXCL16}}.
