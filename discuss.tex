\subsection{Why do the project-specific correlation fitting layers work? }
Firstly, we explore the reason why the project-specific correlation fitting layers in the \TRANPCNN model. The key part of \TRANPCNN is the project-specific correlation fitting layers, and in this section, we discuss why the project-specific correlation fitting layers work. 

The only difference between NP-CNN and \TRANPCNN is that the \TRANPCNN applies project-specific correlation fitting layers, which is particularly designed for cross-project bug localization to deal with the situation that the distribution of data from source project and target project are inconsistent, leading to a bias results if the prediction structure is the same. The \TRANPCNN model employs two fully-connected network for bug localization prediction, one is for source project and the other one is used for target project. This structure overcomes the problem that the model training from two projects affected from each other. During training process, the data of source project uses the CNN model in the transferable feature extraction layers  for feature extraction and employs the fully-connected network $fc^s$ for prediction training, and the target project data is trained using the same CNN but predicted with fully-connected network $fc^t$. This process helps improve the bug localization performance from target project by enjoying the advantage in sharing the same network of transferable feature extraction process from source project, and meanwhile adapting prediction network using training data from target project. 

\subsection{Why does \TRANPCNN improve the bug localization performance?}
The reason why \TRANPCNN improve the bug localization performance can be summarized as 4 folds:
\begin{itemize}
\item The transferable feature extraction layers are able to generate a more semantic representation of source code. The transferable feature extraction layers are able to extract the semantic features reflecting the structural and sequential nature, leading to a high-level representation of source code. In addition, the results in Table~\ref{tab:results3} that TCA+$^{(D)}$ (TCA+ with deep features) outperforms TCA+$^{(P)}$ (TCA+ with traditional features), which show that the deep features generated by transferable feature extraction layers are able to improve the performance of cross-project bug localization performance. 
\item The transferable feature extraction layers can improve the performance in cross-project bug localization. As aforementioned, since source project and target project use the same programming language, the semantic feature extraction rule of cross-project is similar, which means that the semantic feature extraction sub-structure from source project could be transferable to the target model. The comparison results between \TRANPCNN and NPCNN have supported this reason. 
\item The project-specific correlation fitting layers are effective for cross-project bug localization. The reason has been explained in the last subsection that the project-specific correlation fitting layers help counter the inconsistent distribution problems in cross-project bug localization task by employing two fully-connected network for predicting adaptation.
\item \TRANPCNN can fully exploit the advantage in using the labeled data from target project. A few data (20\% in our experiments) from the target project has labels. \TRANPCNN is able to make better use of labeled data from target project in training the fully-connected network $fc^t$ for prediction during the training process. However, traditional transfer model TCA+ has not fully used the labeled data in the target project from transfer learning view. 
\end{itemize}



\subsection{Threats to Validity}

There are three kinds of threat that may impact the validity of this study: threats to internal validity, threats to external validity, and threats to construct validity. We acknowledge these threats below.

Threats to internal validity relate to author bias and errors in our code and experiments. We have checked our code for bugs and fixed any that we can identify. There may still be errors that we do not notice though. The dataset that we obtain are taken from prior papers~\cite{zhou2012should,KochharTL14} and have been used to evaluate other bug localization techniques, e.g.,~\cite{zhou2012should,SahaLKP14,huo2016learning}. The data are bug reports taken from bug tracking systems from real projects (i.e., Lucene-Java) and thus are realistic. Thus, we believe there are limited threats to the internal validity of the study. 

Threats to external validity relate to the generalizability of the study. We have analyzed data that includes 793 bug reports taken from three projects. Admittedly, the projects that we analyze may not represent all the projects out there. However, to the best of our knowledge, there are no other bug localization dataset that is absent from biases identified by Herzig et al.~\cite{HerzigJZ13} and Kochhar et al~\cite{KochharTL14}. In the future, we plan to reduce the threats to external validity by investigating more bug reports that are absent from those biases. %Still, our threats to external validity are less than existing bug localization work since the amount of data that we investigate is larger than prior work. For example, Zhou et al. only use ... bug reports from ... projects~\cite{zhou2012should}, Saha et al. only use ... bug reports from ... reports~\cite{SahaLKP14}, and Huo et al. only use ... bug reports from ... projects~\cite{huo2016learning}. In a future work, we plan to reduce the threats to external validity further by investigating more bug reports from additional projects.

Threats to construct validity relate to the suitability of our evaluation metrics. We have used Top-N, MAP, and MRR as evaluation metrics. These metrics were also used by prior bug localization studies, e.g.,  ~\cite{zhou2012should,SahaLKP14,huo2016learning}. Thus, we believe there are limited threats to construct validity. 