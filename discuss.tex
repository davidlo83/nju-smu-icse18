\subsection{Why do the project-specific correlation fitting layers work? }
We explore the reason why the project-specific correlation fitting layer in \TRANPCNN works well. The only difference between NP-CNN and \TRANPCNN is that \TRANPCNN applies project-specific correlation fitting layer, which is particularly designed for cross-project bug localization and deals with the situation where data distributions in a source and a target project are different, which may lead to a biased results if the prediction structure is assumed to be the same. \TRANPCNN employs two fully-connected networks for bug localization, one is for the source project and the other is for the target project. This structure overcomes the problem where data from two different projects affected one another. During training, the data from the source project is fed to the transferable feature extraction layer and the output is then fed to the fully-connected network $fc^s$, and the data from the target project is fed to the same transferable feature extraction layer but the output is now fed to the fully-connected network $fc^t$. This process improves cross-project bug localization performance by sharing the same transferable feature extraction layer for source and target project and also adapting prediction network using training data from target project. 

\subsection{Why does \TRANPCNN improve the bug localization performance?}
The reason why \TRANPCNN improve the bug localization performance can be summarized in 4 folds:
\begin{itemize}
\item The transferable feature extraction layer can generate a better representation of source code in the form of deep semantic features that reflect the structural and sequential nature of source code. In addition, the results in Table~\ref{tab:results3} show that TCA+$^{(D)}$ (TCA+ with deep features) outperforms TCA+$^{(P)}$ (TCA+ with traditional features), which show that the features generated by the transferable feature extraction layer improves the performance of cross-project bug localization. 
\item The transferable feature extraction layers can improve the performance of cross-project bug localization. Since source and target project use the same programming language, the semantic feature extraction rule between the two projects should be the same, which means that the semantic feature extraction sub-structure from the source project could be transferable to the target project. The comparison results between \TRANPCNN and NPCNN have supported this. 
\item The project-specific correlation fitting layer is effective for cross-project bug localization. It helps counter the different distribution problem in cross-project bug localization by employing two fully-connected networks for adapting the prediction function.
\item \TRANPCNN can fully exploit the advantage of using labeled data from the target project. A partial data (20\% in our experiments) from the target project contain labels. \TRANPCNN is capable to make better use of labeled data from the target project by using the fully-connected network $fc^t$ for learning prediction function. On the other hand, traditional transfer model like TCA+ has not fully utilized labeled data from the target project. 
\end{itemize}



\subsection{Threats to Validity}

There are three kinds of threat that may impact the validity of this study: threats to internal validity, threats to external validity, and threats to construct validity. We acknowledge these threats below.

Threats to internal validity relate to author bias and errors in our code and experiments. We have checked our code for bugs and fixed any that we can identify. There may still be errors that we do not notice though. The dataset that we obtain are taken from prior papers~\cite{zhou2012should,KochharTL14} and have been used to evaluate other bug localization techniques, e.g.,~\cite{zhou2012should,SahaLKP14,huo2016learning}. The data are bug reports taken from bug tracking systems from real projects (i.e., Lucene-Java) and thus are realistic. Thus, we believe there are limited threats to the internal validity of the study. 

Threats to external validity relate to the generalizability of the study. We have analyzed data that includes 793 bug reports taken from three projects. Admittedly, the projects that we analyze may not represent all the projects out there. However, to the best of our knowledge, there are no other bug localization dataset that is absent from biases identified by Herzig et al.~\cite{HerzigJZ13} and Kochhar et al~\cite{KochharTL14}. In the future, we plan to reduce the threats to external validity by investigating more bug reports that are absent from those biases. %Still, our threats to external validity are less than existing bug localization work since the amount of data that we investigate is larger than prior work. For example, Zhou et al. only use ... bug reports from ... projects~\cite{zhou2012should}, Saha et al. only use ... bug reports from ... reports~\cite{SahaLKP14}, and Huo et al. only use ... bug reports from ... projects~\cite{huo2016learning}. In a future work, we plan to reduce the threats to external validity further by investigating more bug reports from additional projects.

Threats to construct validity relate to the suitability of our evaluation metrics. We have used Top-N, MAP, and MRR as evaluation metrics. These metrics were also used by prior bug localization studies, e.g.,  ~\cite{zhou2012should,SahaLKP14,huo2016learning}. Thus, we believe there are limited threats to construct validity. 