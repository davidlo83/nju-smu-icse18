\subsection{Why do the heterogeneous predicting adaptation layers work? }
Firstly, we explore the reason why the heterogeneous prediction adaptation layers work in the \TRANPCNN model. The key part of \TRANPCNN is the heterogeneous predicting adaptation layers, and in this section, we discuss why the heterogeneous predicting adaptation layers work. 

The only difference between NP-CNN and \TRANPCNN is that the \TRANPCNN applies heterogeneous predicting adaptation layers, which is particularly designed for cross-project bug localization to deal with the situation that the distribution of data from source project and target project may be different, leading to a bias results if the prediction structure is the same. The \TRANPCNN model employs two fully-connected network for bug localization prediction, one is for source project and the other one is used for target project. This structure overcomes the problem that the model training from two projects affected from each other. During training process, the data of source project uses the CNN model in the transferable feature extraction layers  for feature extraction and employs the fully-connected network $fc_s$ for prediction training, and the target project data is trained using the same CNN but predicted with fully-connected network $fc_t$. This process helps improve the bug localization performance from target project by enjoying the advantage in sharing the same network of transferable feature extraction process from source project, and meanwhile adapting prediction network using training data from target project. 

From the experiments results, we still find that SimpleTrans outperforms NP-CNN in terms of most evaluation metrics. SimpleTrans has the same structure with NP-CNN, but fine tune the parameters of fully-connected network using target project data, which further shows that using target project data to adjust the weight in the fully-connected network for prediction is effective for bug localization. \TRANPCNN outperforms NPCNN and SimpleTrans shows that the heterogeneous predicting adaptation layers are able to improve the performance of bug localization. 

\subsection{Why does \TRANPCNN improve the bug localization performance?}
The reason why \TRANPCNN improve the bug localization performance can be summarized as 4 folds:
\begin{itemize}
\item The transferable feature extraction layers are able to generate a more semantic representation of source code. The transferable feature extraction layers are able to extract the semantic features reflecting the structural and sequential nature, leading to a high-level representation of source code. In addition, the results in Table~\ref{tab:results3} that TCA-D (TCA with deep features) outperforms TCA-P (TCA with traditional features), which show that the deep features generated by transferable feature extraction layers are able to improve the performance of cross-project bug localization performance. 
\item The transferable feature extraction layers can improve the performance in cross-project bug localization. As aforementioned, since source project and target project use the same programming language, the semantic feature extraction rule of cross-project is similar, which means that the semantic feature extraction sub-structure from source project could be transferable to the target model. The comparison results between \TRANPCNN and NPCNN have supported this reason. 
\item The heterogeneous predicting adaptation layers are effective for cross-project bug localization. The reason has been explained in the last subsection that the heterogeneous predicting adaptation layers help counter the inconsistent distribution problems in cross-project bug localization task by employing two fully-connected network for predicting adaptation.
\item \TRANPCNN can fully exploit the advantage in using the labeled data from target project. A few data (20\% in our experiments) from the target project has labels. \TRANPCNN is able to make better use of labeled data from target project in training the fully-connected network $fc_t$ for prediction during the training process. However, traditional transfer model TCA+ has not fully used the labeled data in the target project from transfer learning view. 
\end{itemize}



\subsection{Threats to Validity}

\dl{Need to complete when the dataset used is finalized.}

There are three kinds of threat that may impact the validity of this study: threats to internal validity, threats to external validity, and threats to construct validity. We acknowledge these threats below.

Threats to internal validity relate to author bias and errors in our code and experiments. We have checked our code for bugs and fixed any that we can identify. There may still be errors that we do not notice though. The dataset that we obtain are taken from prior papers~\cite{zhou2012should,KochharTL14} and have been used to evaluate other bug localization techniques, e.g.,~\cite{zhou2012should,SahaLKP14,huo2016learning}. The data are bug reports taken from bug tracking systems from real projects (i.e., ....) and thus are realistic. Thus, we believe there are limited threats to the internal validity of the study. 

Threats to external validity relate to the generalizability of the study. We have analyzed data that includes ... bug reports taken from ... projects. Admittedly, the projects that we analyze may not represent all the projects out there. Still, our threats to external validity are less than existing bug localization work since the amount of data that we investigate is larger than prior work. For example, Zhou et al. only use ... bug reports from ... projects~\cite{zhou2012should}, Saha et al. only use ... bug reports from ... reports~\cite{SahaLKP14}, and Huo et al. only use ... bug reports from ... projects~\cite{huo2016learning}. In a future work, we plan to reduce the threats to external validity further by investigating more bug reports from additional projects.

Threats to construct validity relate to the suitability of our evaluation metrics. We have used ... as evaluation metrics. These metrics were also used by prior bug localization studies, e.g.,  ~\cite{zhou2012should,SahaLKP14,huo2016learning}. Thus, we believe there are limited threats to construct validity. 