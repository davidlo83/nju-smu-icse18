%The evaluation metrics are presented here. %\dl{Ferdian, please add details on evaluation metrics.}

Following prior bug localization studies~\cite{SahaLKP13,SahaLKP14,zhou2012should,huo2016learning}, we consider three evaluation metrics: Top-N, Mean Average Precision (MAP), and Mean Reciprocal Rank. Their brief definitions considering the context of bug localization are given below:

\vspace{0.2cm}\noindent{\bf Top-N.} Top-N reports the proportion of bug reports for which one of the buggy files appear in the top-N position in the ranked list returned by a bug localization tool. Following prior studies~\cite{SahaLKP13,SahaLKP14,zhou2012should,huo2016learning}, we consider three values of N, i.e., 1, 5, and 10. This is further motivated by the findings of Kochhar et al.~\cite{KochharXLL16} which highlight that more than 95\% of practitioners would not check beyond the top-10 results of a bug localization tool.

\vspace{0.2cm}\noindent{\bf Mean Average Precision (MAP).} For each ranked list produced by a bug localization technique for a bug report, we can compute its average precision (AP) as follows:

\begin{equation}
\sum_{i} \frac{P(i)\times isBuggy(i)}{\# buggy files}
\end{equation}

In the above equation, $\mathit{P(i)}$ is the precision at position $i$ (i.e., proportion of files at position 1 to $i$ that are buggy), while $mathit{isBuggy(i)}$ is 1 if the file at position $i$ is buggy and 0 otherwise. The denominator of the equation is the number of buggy files in the entire ranked list. MAP is the mean of the APs considering all bug reports.

\vspace{0.2cm}\noindent{\bf Mean Reciprocal Rank (MRR).} For each ranked list produced by a bug localization technique for a bug report, we can compute the position of the first file that is buggy. The reciprocal of such position is referred to as the reciprocal rank (RR). MRR is the mean of the RRs considering all bug reports.

